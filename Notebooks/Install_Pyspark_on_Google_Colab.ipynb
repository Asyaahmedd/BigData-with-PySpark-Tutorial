{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Pyspark on Google Colab"
      ],
      "metadata": {
        "id": "3lC3Wbk8Bov4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Download Java because Spark requires Java Virtual Machine (JVM)."
      ],
      "metadata": {
        "id": "HeS-mX3rBg5Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yjYTAX2wBC1x"
      },
      "outputs": [],
      "source": [
        "# Install Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Download the latest version of the Apache Spark using this link [https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)"
      ],
      "metadata": {
        "id": "FZAIv9ODDtS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download Spark\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "BfEPRseBBmb0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the file\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "vy06-CoeD3h3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Set up the environment for Spark."
      ],
      "metadata": {
        "id": "mnYeL-WIEsrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = '/content/spark-3.5.0-bin-hadoop3'"
      ],
      "metadata": {
        "id": "lje8HSi7EA0_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Install and import the library for locating Spark."
      ],
      "metadata": {
        "id": "3rTqFRTQE8oL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install library for finding Spark\n",
        "!pip install -q findspark\n"
      ],
      "metadata": {
        "id": "df3VLOlDE5jR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark# Initiate findspark\n",
        "findspark.init()# Check the location for Spark\n",
        "findspark.find()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kFtS805qFFo7",
        "outputId": "214adcbf-ee8c-460d-b011-c3a1144c3d18"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.5.0-bin-hadoop3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Start a Spark session, and check the session information."
      ],
      "metadata": {
        "id": "8jGnPjxZFQ0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SparkSession\n",
        "from pyspark.sql import SparkSession# Create a Spark Session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()# Check Spark Session Information\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "iSwW8QpNFK1r",
        "outputId": "904af311-ce56-45d5-ffc2-0ced8a72a800"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7dc5bd927130>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://186f4df8319e:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Test if Spark is installed successfully by importing a Spark library."
      ],
      "metadata": {
        "id": "5V8zqy9MFcc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import a Spark function from library\n",
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "ouTxwUZSFXxZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no error message after running the code, indicating that Spark is successfully installed."
      ],
      "metadata": {
        "id": "qRzl4hzFFwrG"
      }
    }
  ]
}